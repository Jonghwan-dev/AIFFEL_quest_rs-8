{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "09b92f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "import numpy as np\n",
    "\n",
    "data = load_diabetes()\n",
    "df_X = np.array(data.data)\n",
    "df_y = np.array(data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf3465fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "(442,)\n"
     ]
    }
   ],
   "source": [
    "print(df_X.shape)\n",
    "print(df_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dfa93c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.2, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1836c1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.rand(10)\n",
    "b = np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c00f630b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.26402396 0.52038998 0.65547386 0.32769005 0.94562925 0.17549958\n",
      " 0.61995521 0.47415461 0.54782359 0.85051438]\n",
      "0.6910550244644443\n"
     ]
    }
   ],
   "source": [
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "279e3549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, W, b):\n",
    "    prediction = 0\n",
    "    for i in range(10):\n",
    "        prediction += X[:, i] * W[i]\n",
    "    prediction += b\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f4507284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(a, b):\n",
    "    mse = ((a - b) ** 2).mean()\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ea6aa91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, W, b, y):\n",
    "    predictions = model(X, W, b)\n",
    "    L = MSE(predictions, y)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1c7694e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, W, b, y):\n",
    "    N = len(y)\n",
    "    y_pred = model(X, W, b)\n",
    "    dW = 1/N * 2 * X.T.dot(y_pred - y)\n",
    "    db = 2 * (y_pred - y).mean()\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c5c6526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b03e73ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 : Loss 2926.0636\n",
      "Iteration 20 : Loss 2924.9661\n",
      "Iteration 30 : Loss 2923.8786\n",
      "Iteration 40 : Loss 2922.8009\n",
      "Iteration 50 : Loss 2921.7330\n",
      "Iteration 60 : Loss 2920.6746\n",
      "Iteration 70 : Loss 2919.6258\n",
      "Iteration 80 : Loss 2918.5864\n",
      "Iteration 90 : Loss 2917.5564\n",
      "Iteration 100 : Loss 2916.5356\n",
      "Iteration 110 : Loss 2915.5240\n",
      "Iteration 120 : Loss 2914.5214\n",
      "Iteration 130 : Loss 2913.5279\n",
      "Iteration 140 : Loss 2912.5432\n",
      "Iteration 150 : Loss 2911.5674\n",
      "Iteration 160 : Loss 2910.6002\n",
      "Iteration 170 : Loss 2909.6418\n",
      "Iteration 180 : Loss 2908.6919\n",
      "Iteration 190 : Loss 2907.7504\n",
      "Iteration 200 : Loss 2906.8174\n",
      "Iteration 210 : Loss 2905.8927\n",
      "Iteration 220 : Loss 2904.9762\n",
      "Iteration 230 : Loss 2904.0678\n",
      "Iteration 240 : Loss 2903.1676\n",
      "Iteration 250 : Loss 2902.2754\n",
      "Iteration 260 : Loss 2901.3910\n",
      "Iteration 270 : Loss 2900.5146\n",
      "Iteration 280 : Loss 2899.6459\n",
      "Iteration 290 : Loss 2898.7849\n",
      "Iteration 300 : Loss 2897.9315\n",
      "Iteration 310 : Loss 2897.0857\n",
      "Iteration 320 : Loss 2896.2474\n",
      "Iteration 330 : Loss 2895.4165\n",
      "Iteration 340 : Loss 2894.5929\n",
      "Iteration 350 : Loss 2893.7766\n",
      "Iteration 360 : Loss 2892.9675\n",
      "Iteration 370 : Loss 2892.1656\n",
      "Iteration 380 : Loss 2891.3707\n",
      "Iteration 390 : Loss 2890.5828\n",
      "Iteration 400 : Loss 2889.8019\n",
      "Iteration 410 : Loss 2889.0278\n",
      "Iteration 420 : Loss 2888.2605\n",
      "Iteration 430 : Loss 2887.5000\n",
      "Iteration 440 : Loss 2886.7462\n",
      "Iteration 450 : Loss 2885.9990\n",
      "Iteration 460 : Loss 2885.2583\n",
      "Iteration 470 : Loss 2884.5242\n",
      "Iteration 480 : Loss 2883.7964\n",
      "Iteration 490 : Loss 2883.0751\n",
      "Iteration 500 : Loss 2882.3600\n",
      "Iteration 510 : Loss 2881.6512\n",
      "Iteration 520 : Loss 2880.9487\n",
      "Iteration 530 : Loss 2880.2522\n",
      "Iteration 540 : Loss 2879.5618\n",
      "Iteration 550 : Loss 2878.8775\n",
      "Iteration 560 : Loss 2878.1991\n",
      "Iteration 570 : Loss 2877.5267\n",
      "Iteration 580 : Loss 2876.8601\n",
      "Iteration 590 : Loss 2876.1993\n",
      "Iteration 600 : Loss 2875.5443\n",
      "Iteration 610 : Loss 2874.8950\n",
      "Iteration 620 : Loss 2874.2513\n",
      "Iteration 630 : Loss 2873.6132\n",
      "Iteration 640 : Loss 2872.9806\n",
      "Iteration 650 : Loss 2872.3536\n",
      "Iteration 660 : Loss 2871.7319\n",
      "Iteration 670 : Loss 2871.1157\n",
      "Iteration 680 : Loss 2870.5048\n",
      "Iteration 690 : Loss 2869.8992\n",
      "Iteration 700 : Loss 2869.2989\n",
      "Iteration 710 : Loss 2868.7037\n",
      "Iteration 720 : Loss 2868.1137\n",
      "Iteration 730 : Loss 2867.5288\n",
      "Iteration 740 : Loss 2866.9490\n",
      "Iteration 750 : Loss 2866.3741\n",
      "Iteration 760 : Loss 2865.8042\n",
      "Iteration 770 : Loss 2865.2393\n",
      "Iteration 780 : Loss 2864.6792\n",
      "Iteration 790 : Loss 2864.1239\n",
      "Iteration 800 : Loss 2863.5734\n",
      "Iteration 810 : Loss 2863.0277\n",
      "Iteration 820 : Loss 2862.4866\n",
      "Iteration 830 : Loss 2861.9502\n",
      "Iteration 840 : Loss 2861.4184\n",
      "Iteration 850 : Loss 2860.8912\n",
      "Iteration 860 : Loss 2860.3685\n",
      "Iteration 870 : Loss 2859.8503\n",
      "Iteration 880 : Loss 2859.3365\n",
      "Iteration 890 : Loss 2858.8272\n",
      "Iteration 900 : Loss 2858.3222\n",
      "Iteration 910 : Loss 2857.8215\n",
      "Iteration 920 : Loss 2857.3251\n",
      "Iteration 930 : Loss 2856.8330\n",
      "Iteration 940 : Loss 2856.3450\n",
      "Iteration 950 : Loss 2855.8613\n",
      "Iteration 960 : Loss 2855.3816\n",
      "Iteration 970 : Loss 2854.9061\n",
      "Iteration 980 : Loss 2854.4346\n",
      "Iteration 990 : Loss 2853.9672\n",
      "Iteration 1000 : Loss 2853.5037\n",
      "Iteration 1010 : Loss 2853.0442\n",
      "Iteration 1020 : Loss 2852.5886\n",
      "Iteration 1030 : Loss 2852.1368\n",
      "Iteration 1040 : Loss 2851.6890\n",
      "Iteration 1050 : Loss 2851.2449\n",
      "Iteration 1060 : Loss 2850.8046\n",
      "Iteration 1070 : Loss 2850.3680\n",
      "Iteration 1080 : Loss 2849.9351\n",
      "Iteration 1090 : Loss 2849.5060\n",
      "Iteration 1100 : Loss 2849.0804\n",
      "Iteration 1110 : Loss 2848.6585\n",
      "Iteration 1120 : Loss 2848.2401\n",
      "Iteration 1130 : Loss 2847.8253\n",
      "Iteration 1140 : Loss 2847.4140\n",
      "Iteration 1150 : Loss 2847.0061\n",
      "Iteration 1160 : Loss 2846.6018\n",
      "Iteration 1170 : Loss 2846.2008\n",
      "Iteration 1180 : Loss 2845.8032\n",
      "Iteration 1190 : Loss 2845.4090\n",
      "Iteration 1200 : Loss 2845.0181\n",
      "Iteration 1210 : Loss 2844.6305\n",
      "Iteration 1220 : Loss 2844.2462\n",
      "Iteration 1230 : Loss 2843.8651\n",
      "Iteration 1240 : Loss 2843.4872\n",
      "Iteration 1250 : Loss 2843.1125\n",
      "Iteration 1260 : Loss 2842.7409\n",
      "Iteration 1270 : Loss 2842.3725\n",
      "Iteration 1280 : Loss 2842.0072\n",
      "Iteration 1290 : Loss 2841.6449\n",
      "Iteration 1300 : Loss 2841.2857\n",
      "Iteration 1310 : Loss 2840.9295\n",
      "Iteration 1320 : Loss 2840.5763\n",
      "Iteration 1330 : Loss 2840.2260\n",
      "Iteration 1340 : Loss 2839.8787\n",
      "Iteration 1350 : Loss 2839.5343\n",
      "Iteration 1360 : Loss 2839.1928\n",
      "Iteration 1370 : Loss 2838.8541\n",
      "Iteration 1380 : Loss 2838.5182\n",
      "Iteration 1390 : Loss 2838.1852\n",
      "Iteration 1400 : Loss 2837.8549\n",
      "Iteration 1410 : Loss 2837.5275\n",
      "Iteration 1420 : Loss 2837.2027\n",
      "Iteration 1430 : Loss 2836.8806\n",
      "Iteration 1440 : Loss 2836.5613\n",
      "Iteration 1450 : Loss 2836.2445\n",
      "Iteration 1460 : Loss 2835.9305\n",
      "Iteration 1470 : Loss 2835.6190\n",
      "Iteration 1480 : Loss 2835.3101\n",
      "Iteration 1490 : Loss 2835.0038\n",
      "Iteration 1500 : Loss 2834.7001\n",
      "Iteration 1510 : Loss 2834.3989\n",
      "Iteration 1520 : Loss 2834.1001\n",
      "Iteration 1530 : Loss 2833.8039\n",
      "Iteration 1540 : Loss 2833.5101\n",
      "Iteration 1550 : Loss 2833.2187\n",
      "Iteration 1560 : Loss 2832.9298\n",
      "Iteration 1570 : Loss 2832.6432\n",
      "Iteration 1580 : Loss 2832.3590\n",
      "Iteration 1590 : Loss 2832.0772\n",
      "Iteration 1600 : Loss 2831.7977\n",
      "Iteration 1610 : Loss 2831.5205\n",
      "Iteration 1620 : Loss 2831.2456\n",
      "Iteration 1630 : Loss 2830.9730\n",
      "Iteration 1640 : Loss 2830.7026\n",
      "Iteration 1650 : Loss 2830.4344\n",
      "Iteration 1660 : Loss 2830.1685\n",
      "Iteration 1670 : Loss 2829.9047\n",
      "Iteration 1680 : Loss 2829.6432\n",
      "Iteration 1690 : Loss 2829.3837\n",
      "Iteration 1700 : Loss 2829.1264\n",
      "Iteration 1710 : Loss 2828.8712\n",
      "Iteration 1720 : Loss 2828.6182\n",
      "Iteration 1730 : Loss 2828.3672\n",
      "Iteration 1740 : Loss 2828.1182\n",
      "Iteration 1750 : Loss 2827.8713\n",
      "Iteration 1760 : Loss 2827.6264\n",
      "Iteration 1770 : Loss 2827.3835\n",
      "Iteration 1780 : Loss 2827.1427\n",
      "Iteration 1790 : Loss 2826.9037\n",
      "Iteration 1800 : Loss 2826.6668\n",
      "Iteration 1810 : Loss 2826.4318\n",
      "Iteration 1820 : Loss 2826.1987\n",
      "Iteration 1830 : Loss 2825.9675\n",
      "Iteration 1840 : Loss 2825.7382\n",
      "Iteration 1850 : Loss 2825.5107\n",
      "Iteration 1860 : Loss 2825.2851\n",
      "Iteration 1870 : Loss 2825.0614\n",
      "Iteration 1880 : Loss 2824.8395\n",
      "Iteration 1890 : Loss 2824.6194\n",
      "Iteration 1900 : Loss 2824.4011\n",
      "Iteration 1910 : Loss 2824.1845\n",
      "Iteration 1920 : Loss 2823.9697\n",
      "Iteration 1930 : Loss 2823.7567\n",
      "Iteration 1940 : Loss 2823.5454\n",
      "Iteration 1950 : Loss 2823.3358\n",
      "Iteration 1960 : Loss 2823.1279\n",
      "Iteration 1970 : Loss 2822.9217\n",
      "Iteration 1980 : Loss 2822.7171\n",
      "Iteration 1990 : Loss 2822.5142\n",
      "Iteration 2000 : Loss 2822.3130\n",
      "Iteration 2010 : Loss 2822.1134\n",
      "Iteration 2020 : Loss 2821.9154\n",
      "Iteration 2030 : Loss 2821.7190\n",
      "Iteration 2040 : Loss 2821.5242\n",
      "Iteration 2050 : Loss 2821.3310\n",
      "Iteration 2060 : Loss 2821.1393\n",
      "Iteration 2070 : Loss 2820.9491\n",
      "Iteration 2080 : Loss 2820.7605\n",
      "Iteration 2090 : Loss 2820.5735\n",
      "Iteration 2100 : Loss 2820.3879\n",
      "Iteration 2110 : Loss 2820.2038\n",
      "Iteration 2120 : Loss 2820.0212\n",
      "Iteration 2130 : Loss 2819.8401\n",
      "Iteration 2140 : Loss 2819.6604\n",
      "Iteration 2150 : Loss 2819.4821\n",
      "Iteration 2160 : Loss 2819.3053\n",
      "Iteration 2170 : Loss 2819.1300\n",
      "Iteration 2180 : Loss 2818.9560\n",
      "Iteration 2190 : Loss 2818.7834\n",
      "Iteration 2200 : Loss 2818.6122\n",
      "Iteration 2210 : Loss 2818.4423\n",
      "Iteration 2220 : Loss 2818.2739\n",
      "Iteration 2230 : Loss 2818.1067\n",
      "Iteration 2240 : Loss 2817.9409\n",
      "Iteration 2250 : Loss 2817.7765\n",
      "Iteration 2260 : Loss 2817.6133\n",
      "Iteration 2270 : Loss 2817.4514\n",
      "Iteration 2280 : Loss 2817.2909\n",
      "Iteration 2290 : Loss 2817.1316\n",
      "Iteration 2300 : Loss 2816.9736\n",
      "Iteration 2310 : Loss 2816.8168\n",
      "Iteration 2320 : Loss 2816.6613\n",
      "Iteration 2330 : Loss 2816.5070\n",
      "Iteration 2340 : Loss 2816.3539\n",
      "Iteration 2350 : Loss 2816.2021\n",
      "Iteration 2360 : Loss 2816.0515\n",
      "Iteration 2370 : Loss 2815.9020\n",
      "Iteration 2380 : Loss 2815.7538\n",
      "Iteration 2390 : Loss 2815.6067\n",
      "Iteration 2400 : Loss 2815.4608\n",
      "Iteration 2410 : Loss 2815.3160\n",
      "Iteration 2420 : Loss 2815.1724\n",
      "Iteration 2430 : Loss 2815.0299\n",
      "Iteration 2440 : Loss 2814.8886\n",
      "Iteration 2450 : Loss 2814.7483\n",
      "Iteration 2460 : Loss 2814.6092\n",
      "Iteration 2470 : Loss 2814.4712\n",
      "Iteration 2480 : Loss 2814.3342\n",
      "Iteration 2490 : Loss 2814.1983\n",
      "Iteration 2500 : Loss 2814.0635\n",
      "Iteration 2510 : Loss 2813.9298\n",
      "Iteration 2520 : Loss 2813.7971\n",
      "Iteration 2530 : Loss 2813.6655\n",
      "Iteration 2540 : Loss 2813.5349\n",
      "Iteration 2550 : Loss 2813.4053\n",
      "Iteration 2560 : Loss 2813.2767\n",
      "Iteration 2570 : Loss 2813.1491\n",
      "Iteration 2580 : Loss 2813.0226\n",
      "Iteration 2590 : Loss 2812.8970\n",
      "Iteration 2600 : Loss 2812.7724\n",
      "Iteration 2610 : Loss 2812.6488\n",
      "Iteration 2620 : Loss 2812.5262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2630 : Loss 2812.4045\n",
      "Iteration 2640 : Loss 2812.2837\n",
      "Iteration 2650 : Loss 2812.1639\n",
      "Iteration 2660 : Loss 2812.0451\n",
      "Iteration 2670 : Loss 2811.9271\n",
      "Iteration 2680 : Loss 2811.8101\n",
      "Iteration 2690 : Loss 2811.6940\n",
      "Iteration 2700 : Loss 2811.5788\n",
      "Iteration 2710 : Loss 2811.4645\n",
      "Iteration 2720 : Loss 2811.3511\n",
      "Iteration 2730 : Loss 2811.2385\n",
      "Iteration 2740 : Loss 2811.1269\n",
      "Iteration 2750 : Loss 2811.0161\n",
      "Iteration 2760 : Loss 2810.9061\n",
      "Iteration 2770 : Loss 2810.7970\n",
      "Iteration 2780 : Loss 2810.6888\n",
      "Iteration 2790 : Loss 2810.5814\n",
      "Iteration 2800 : Loss 2810.4748\n",
      "Iteration 2810 : Loss 2810.3691\n",
      "Iteration 2820 : Loss 2810.2641\n",
      "Iteration 2830 : Loss 2810.1600\n",
      "Iteration 2840 : Loss 2810.0567\n",
      "Iteration 2850 : Loss 2809.9542\n",
      "Iteration 2860 : Loss 2809.8524\n",
      "Iteration 2870 : Loss 2809.7515\n",
      "Iteration 2880 : Loss 2809.6513\n",
      "Iteration 2890 : Loss 2809.5519\n",
      "Iteration 2900 : Loss 2809.4533\n",
      "Iteration 2910 : Loss 2809.3554\n",
      "Iteration 2920 : Loss 2809.2583\n",
      "Iteration 2930 : Loss 2809.1619\n",
      "Iteration 2940 : Loss 2809.0662\n",
      "Iteration 2950 : Loss 2808.9713\n",
      "Iteration 2960 : Loss 2808.8771\n",
      "Iteration 2970 : Loss 2808.7837\n",
      "Iteration 2980 : Loss 2808.6909\n",
      "Iteration 2990 : Loss 2808.5989\n",
      "Iteration 3000 : Loss 2808.5075\n",
      "Iteration 3010 : Loss 2808.4169\n",
      "Iteration 3020 : Loss 2808.3270\n",
      "Iteration 3030 : Loss 2808.2377\n",
      "Iteration 3040 : Loss 2808.1491\n",
      "Iteration 3050 : Loss 2808.0612\n",
      "Iteration 3060 : Loss 2807.9740\n",
      "Iteration 3070 : Loss 2807.8874\n",
      "Iteration 3080 : Loss 2807.8015\n",
      "Iteration 3090 : Loss 2807.7162\n",
      "Iteration 3100 : Loss 2807.6316\n",
      "Iteration 3110 : Loss 2807.5476\n",
      "Iteration 3120 : Loss 2807.4643\n",
      "Iteration 3130 : Loss 2807.3816\n",
      "Iteration 3140 : Loss 2807.2995\n",
      "Iteration 3150 : Loss 2807.2180\n",
      "Iteration 3160 : Loss 2807.1372\n",
      "Iteration 3170 : Loss 2807.0569\n",
      "Iteration 3180 : Loss 2806.9773\n",
      "Iteration 3190 : Loss 2806.8983\n",
      "Iteration 3200 : Loss 2806.8199\n",
      "Iteration 3210 : Loss 2806.7420\n",
      "Iteration 3220 : Loss 2806.6648\n",
      "Iteration 3230 : Loss 2806.5881\n",
      "Iteration 3240 : Loss 2806.5120\n",
      "Iteration 3250 : Loss 2806.4365\n",
      "Iteration 3260 : Loss 2806.3615\n",
      "Iteration 3270 : Loss 2806.2871\n",
      "Iteration 3280 : Loss 2806.2133\n",
      "Iteration 3290 : Loss 2806.1400\n",
      "Iteration 3300 : Loss 2806.0673\n",
      "Iteration 3310 : Loss 2805.9951\n",
      "Iteration 3320 : Loss 2805.9235\n",
      "Iteration 3330 : Loss 2805.8523\n",
      "Iteration 3340 : Loss 2805.7818\n",
      "Iteration 3350 : Loss 2805.7117\n",
      "Iteration 3360 : Loss 2805.6422\n",
      "Iteration 3370 : Loss 2805.5732\n",
      "Iteration 3380 : Loss 2805.5047\n",
      "Iteration 3390 : Loss 2805.4367\n",
      "Iteration 3400 : Loss 2805.3692\n",
      "Iteration 3410 : Loss 2805.3022\n",
      "Iteration 3420 : Loss 2805.2357\n",
      "Iteration 3430 : Loss 2805.1698\n",
      "Iteration 3440 : Loss 2805.1043\n",
      "Iteration 3450 : Loss 2805.0393\n",
      "Iteration 3460 : Loss 2804.9747\n",
      "Iteration 3470 : Loss 2804.9107\n",
      "Iteration 3480 : Loss 2804.8471\n",
      "Iteration 3490 : Loss 2804.7840\n",
      "Iteration 3500 : Loss 2804.7213\n",
      "Iteration 3510 : Loss 2804.6592\n",
      "Iteration 3520 : Loss 2804.5975\n",
      "Iteration 3530 : Loss 2804.5362\n",
      "Iteration 3540 : Loss 2804.4754\n",
      "Iteration 3550 : Loss 2804.4150\n",
      "Iteration 3560 : Loss 2804.3551\n",
      "Iteration 3570 : Loss 2804.2956\n",
      "Iteration 3580 : Loss 2804.2366\n",
      "Iteration 3590 : Loss 2804.1780\n",
      "Iteration 3600 : Loss 2804.1198\n",
      "Iteration 3610 : Loss 2804.0620\n",
      "Iteration 3620 : Loss 2804.0047\n",
      "Iteration 3630 : Loss 2803.9478\n",
      "Iteration 3640 : Loss 2803.8913\n",
      "Iteration 3650 : Loss 2803.8352\n",
      "Iteration 3660 : Loss 2803.7796\n",
      "Iteration 3670 : Loss 2803.7243\n",
      "Iteration 3680 : Loss 2803.6695\n",
      "Iteration 3690 : Loss 2803.6150\n",
      "Iteration 3700 : Loss 2803.5609\n",
      "Iteration 3710 : Loss 2803.5073\n",
      "Iteration 3720 : Loss 2803.4540\n",
      "Iteration 3730 : Loss 2803.4011\n",
      "Iteration 3740 : Loss 2803.3486\n",
      "Iteration 3750 : Loss 2803.2965\n",
      "Iteration 3760 : Loss 2803.2447\n",
      "Iteration 3770 : Loss 2803.1934\n",
      "Iteration 3780 : Loss 2803.1424\n",
      "Iteration 3790 : Loss 2803.0917\n",
      "Iteration 3800 : Loss 2803.0415\n",
      "Iteration 3810 : Loss 2802.9916\n",
      "Iteration 3820 : Loss 2802.9420\n",
      "Iteration 3830 : Loss 2802.8929\n",
      "Iteration 3840 : Loss 2802.8440\n",
      "Iteration 3850 : Loss 2802.7956\n",
      "Iteration 3860 : Loss 2802.7474\n",
      "Iteration 3870 : Loss 2802.6997\n",
      "Iteration 3880 : Loss 2802.6522\n",
      "Iteration 3890 : Loss 2802.6051\n",
      "Iteration 3900 : Loss 2802.5584\n",
      "Iteration 3910 : Loss 2802.5120\n",
      "Iteration 3920 : Loss 2802.4659\n",
      "Iteration 3930 : Loss 2802.4201\n",
      "Iteration 3940 : Loss 2802.3747\n",
      "Iteration 3950 : Loss 2802.3296\n",
      "Iteration 3960 : Loss 2802.2848\n",
      "Iteration 3970 : Loss 2802.2404\n",
      "Iteration 3980 : Loss 2802.1962\n",
      "Iteration 3990 : Loss 2802.1524\n",
      "Iteration 4000 : Loss 2802.1089\n",
      "Iteration 4010 : Loss 2802.0657\n",
      "Iteration 4020 : Loss 2802.0228\n",
      "Iteration 4030 : Loss 2801.9802\n",
      "Iteration 4040 : Loss 2801.9379\n",
      "Iteration 4050 : Loss 2801.8959\n",
      "Iteration 4060 : Loss 2801.8542\n",
      "Iteration 4070 : Loss 2801.8128\n",
      "Iteration 4080 : Loss 2801.7717\n",
      "Iteration 4090 : Loss 2801.7309\n",
      "Iteration 4100 : Loss 2801.6904\n",
      "Iteration 4110 : Loss 2801.6501\n",
      "Iteration 4120 : Loss 2801.6102\n",
      "Iteration 4130 : Loss 2801.5705\n",
      "Iteration 4140 : Loss 2801.5311\n",
      "Iteration 4150 : Loss 2801.4920\n",
      "Iteration 4160 : Loss 2801.4532\n",
      "Iteration 4170 : Loss 2801.4146\n",
      "Iteration 4180 : Loss 2801.3763\n",
      "Iteration 4190 : Loss 2801.3383\n",
      "Iteration 4200 : Loss 2801.3005\n",
      "Iteration 4210 : Loss 2801.2630\n",
      "Iteration 4220 : Loss 2801.2258\n",
      "Iteration 4230 : Loss 2801.1888\n",
      "Iteration 4240 : Loss 2801.1521\n",
      "Iteration 4250 : Loss 2801.1157\n",
      "Iteration 4260 : Loss 2801.0795\n",
      "Iteration 4270 : Loss 2801.0435\n",
      "Iteration 4280 : Loss 2801.0078\n",
      "Iteration 4290 : Loss 2800.9724\n",
      "Iteration 4300 : Loss 2800.9372\n",
      "Iteration 4310 : Loss 2800.9022\n",
      "Iteration 4320 : Loss 2800.8675\n",
      "Iteration 4330 : Loss 2800.8330\n",
      "Iteration 4340 : Loss 2800.7988\n",
      "Iteration 4350 : Loss 2800.7648\n",
      "Iteration 4360 : Loss 2800.7311\n",
      "Iteration 4370 : Loss 2800.6975\n",
      "Iteration 4380 : Loss 2800.6642\n",
      "Iteration 4390 : Loss 2800.6312\n",
      "Iteration 4400 : Loss 2800.5983\n",
      "Iteration 4410 : Loss 2800.5657\n",
      "Iteration 4420 : Loss 2800.5333\n",
      "Iteration 4430 : Loss 2800.5012\n",
      "Iteration 4440 : Loss 2800.4692\n",
      "Iteration 4450 : Loss 2800.4375\n",
      "Iteration 4460 : Loss 2800.4060\n",
      "Iteration 4470 : Loss 2800.3747\n",
      "Iteration 4480 : Loss 2800.3436\n",
      "Iteration 4490 : Loss 2800.3128\n",
      "Iteration 4500 : Loss 2800.2821\n",
      "Iteration 4510 : Loss 2800.2517\n",
      "Iteration 4520 : Loss 2800.2215\n",
      "Iteration 4530 : Loss 2800.1914\n",
      "Iteration 4540 : Loss 2800.1616\n",
      "Iteration 4550 : Loss 2800.1320\n",
      "Iteration 4560 : Loss 2800.1026\n",
      "Iteration 4570 : Loss 2800.0734\n",
      "Iteration 4580 : Loss 2800.0444\n",
      "Iteration 4590 : Loss 2800.0155\n",
      "Iteration 4600 : Loss 2799.9869\n",
      "Iteration 4610 : Loss 2799.9585\n",
      "Iteration 4620 : Loss 2799.9302\n",
      "Iteration 4630 : Loss 2799.9022\n",
      "Iteration 4640 : Loss 2799.8743\n",
      "Iteration 4650 : Loss 2799.8467\n",
      "Iteration 4660 : Loss 2799.8192\n",
      "Iteration 4670 : Loss 2799.7919\n",
      "Iteration 4680 : Loss 2799.7648\n",
      "Iteration 4690 : Loss 2799.7378\n",
      "Iteration 4700 : Loss 2799.7111\n",
      "Iteration 4710 : Loss 2799.6845\n",
      "Iteration 4720 : Loss 2799.6581\n",
      "Iteration 4730 : Loss 2799.6319\n",
      "Iteration 4740 : Loss 2799.6059\n",
      "Iteration 4750 : Loss 2799.5800\n",
      "Iteration 4760 : Loss 2799.5543\n",
      "Iteration 4770 : Loss 2799.5288\n",
      "Iteration 4780 : Loss 2799.5035\n",
      "Iteration 4790 : Loss 2799.4783\n",
      "Iteration 4800 : Loss 2799.4533\n",
      "Iteration 4810 : Loss 2799.4284\n",
      "Iteration 4820 : Loss 2799.4037\n",
      "Iteration 4830 : Loss 2799.3792\n",
      "Iteration 4840 : Loss 2799.3549\n",
      "Iteration 4850 : Loss 2799.3307\n",
      "Iteration 4860 : Loss 2799.3066\n",
      "Iteration 4870 : Loss 2799.2828\n",
      "Iteration 4880 : Loss 2799.2590\n",
      "Iteration 4890 : Loss 2799.2355\n",
      "Iteration 4900 : Loss 2799.2121\n",
      "Iteration 4910 : Loss 2799.1888\n",
      "Iteration 4920 : Loss 2799.1657\n",
      "Iteration 4930 : Loss 2799.1428\n",
      "Iteration 4940 : Loss 2799.1200\n",
      "Iteration 4950 : Loss 2799.0973\n",
      "Iteration 4960 : Loss 2799.0749\n",
      "Iteration 4970 : Loss 2799.0525\n",
      "Iteration 4980 : Loss 2799.0303\n",
      "Iteration 4990 : Loss 2799.0082\n",
      "Iteration 5000 : Loss 2798.9863\n",
      "Iteration 5010 : Loss 2798.9646\n",
      "Iteration 5020 : Loss 2798.9429\n",
      "Iteration 5030 : Loss 2798.9214\n",
      "Iteration 5040 : Loss 2798.9001\n",
      "Iteration 5050 : Loss 2798.8789\n",
      "Iteration 5060 : Loss 2798.8578\n",
      "Iteration 5070 : Loss 2798.8369\n",
      "Iteration 5080 : Loss 2798.8161\n",
      "Iteration 5090 : Loss 2798.7954\n",
      "Iteration 5100 : Loss 2798.7749\n",
      "Iteration 5110 : Loss 2798.7545\n",
      "Iteration 5120 : Loss 2798.7342\n",
      "Iteration 5130 : Loss 2798.7141\n",
      "Iteration 5140 : Loss 2798.6941\n",
      "Iteration 5150 : Loss 2798.6742\n",
      "Iteration 5160 : Loss 2798.6545\n",
      "Iteration 5170 : Loss 2798.6349\n",
      "Iteration 5180 : Loss 2798.6154\n",
      "Iteration 5190 : Loss 2798.5960\n",
      "Iteration 5200 : Loss 2798.5767\n",
      "Iteration 5210 : Loss 2798.5576\n",
      "Iteration 5220 : Loss 2798.5386\n",
      "Iteration 5230 : Loss 2798.5197\n",
      "Iteration 5240 : Loss 2798.5010\n",
      "Iteration 5250 : Loss 2798.4823\n",
      "Iteration 5260 : Loss 2798.4638\n",
      "Iteration 5270 : Loss 2798.4454\n",
      "Iteration 5280 : Loss 2798.4271\n",
      "Iteration 5290 : Loss 2798.4090\n",
      "Iteration 5300 : Loss 2798.3909\n",
      "Iteration 5310 : Loss 2798.3730\n",
      "Iteration 5320 : Loss 2798.3551\n",
      "Iteration 5330 : Loss 2798.3374\n",
      "Iteration 5340 : Loss 2798.3198\n",
      "Iteration 5350 : Loss 2798.3023\n",
      "Iteration 5360 : Loss 2798.2849\n",
      "Iteration 5370 : Loss 2798.2677\n",
      "Iteration 5380 : Loss 2798.2505\n",
      "Iteration 5390 : Loss 2798.2334\n",
      "Iteration 5400 : Loss 2798.2165\n",
      "Iteration 5410 : Loss 2798.1996\n",
      "Iteration 5420 : Loss 2798.1829\n",
      "Iteration 5430 : Loss 2798.1663\n",
      "Iteration 5440 : Loss 2798.1497\n",
      "Iteration 5450 : Loss 2798.1333\n",
      "Iteration 5460 : Loss 2798.1170\n",
      "Iteration 5470 : Loss 2798.1008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5480 : Loss 2798.0846\n",
      "Iteration 5490 : Loss 2798.0686\n",
      "Iteration 5500 : Loss 2798.0527\n",
      "Iteration 5510 : Loss 2798.0369\n",
      "Iteration 5520 : Loss 2798.0211\n",
      "Iteration 5530 : Loss 2798.0055\n",
      "Iteration 5540 : Loss 2797.9900\n",
      "Iteration 5550 : Loss 2797.9745\n",
      "Iteration 5560 : Loss 2797.9592\n",
      "Iteration 5570 : Loss 2797.9439\n",
      "Iteration 5580 : Loss 2797.9288\n",
      "Iteration 5590 : Loss 2797.9137\n",
      "Iteration 5600 : Loss 2797.8988\n",
      "Iteration 5610 : Loss 2797.8839\n",
      "Iteration 5620 : Loss 2797.8691\n",
      "Iteration 5630 : Loss 2797.8544\n",
      "Iteration 5640 : Loss 2797.8398\n",
      "Iteration 5650 : Loss 2797.8253\n",
      "Iteration 5660 : Loss 2797.8108\n",
      "Iteration 5670 : Loss 2797.7965\n",
      "Iteration 5680 : Loss 2797.7822\n",
      "Iteration 5690 : Loss 2797.7681\n",
      "Iteration 5700 : Loss 2797.7540\n",
      "Iteration 5710 : Loss 2797.7400\n",
      "Iteration 5720 : Loss 2797.7261\n",
      "Iteration 5730 : Loss 2797.7122\n",
      "Iteration 5740 : Loss 2797.6985\n",
      "Iteration 5750 : Loss 2797.6848\n",
      "Iteration 5760 : Loss 2797.6712\n",
      "Iteration 5770 : Loss 2797.6577\n",
      "Iteration 5780 : Loss 2797.6443\n",
      "Iteration 5790 : Loss 2797.6310\n",
      "Iteration 5800 : Loss 2797.6177\n",
      "Iteration 5810 : Loss 2797.6045\n",
      "Iteration 5820 : Loss 2797.5914\n",
      "Iteration 5830 : Loss 2797.5784\n",
      "Iteration 5840 : Loss 2797.5655\n",
      "Iteration 5850 : Loss 2797.5526\n",
      "Iteration 5860 : Loss 2797.5398\n",
      "Iteration 5870 : Loss 2797.5271\n",
      "Iteration 5880 : Loss 2797.5145\n",
      "Iteration 5890 : Loss 2797.5019\n",
      "Iteration 5900 : Loss 2797.4894\n",
      "Iteration 5910 : Loss 2797.4770\n",
      "Iteration 5920 : Loss 2797.4646\n",
      "Iteration 5930 : Loss 2797.4524\n",
      "Iteration 5940 : Loss 2797.4402\n",
      "Iteration 5950 : Loss 2797.4280\n",
      "Iteration 5960 : Loss 2797.4160\n",
      "Iteration 5970 : Loss 2797.4040\n",
      "Iteration 5980 : Loss 2797.3921\n",
      "Iteration 5990 : Loss 2797.3802\n",
      "Iteration 6000 : Loss 2797.3684\n",
      "Iteration 6010 : Loss 2797.3567\n",
      "Iteration 6020 : Loss 2797.3451\n",
      "Iteration 6030 : Loss 2797.3335\n",
      "Iteration 6040 : Loss 2797.3220\n",
      "Iteration 6050 : Loss 2797.3105\n",
      "Iteration 6060 : Loss 2797.2992\n",
      "Iteration 6070 : Loss 2797.2879\n",
      "Iteration 6080 : Loss 2797.2766\n",
      "Iteration 6090 : Loss 2797.2654\n",
      "Iteration 6100 : Loss 2797.2543\n",
      "Iteration 6110 : Loss 2797.2433\n",
      "Iteration 6120 : Loss 2797.2323\n",
      "Iteration 6130 : Loss 2797.2214\n",
      "Iteration 6140 : Loss 2797.2105\n",
      "Iteration 6150 : Loss 2797.1997\n",
      "Iteration 6160 : Loss 2797.1889\n",
      "Iteration 6170 : Loss 2797.1783\n",
      "Iteration 6180 : Loss 2797.1676\n",
      "Iteration 6190 : Loss 2797.1571\n",
      "Iteration 6200 : Loss 2797.1466\n",
      "Iteration 6210 : Loss 2797.1362\n",
      "Iteration 6220 : Loss 2797.1258\n",
      "Iteration 6230 : Loss 2797.1155\n",
      "Iteration 6240 : Loss 2797.1052\n",
      "Iteration 6250 : Loss 2797.0950\n",
      "Iteration 6260 : Loss 2797.0848\n",
      "Iteration 6270 : Loss 2797.0747\n",
      "Iteration 6280 : Loss 2797.0647\n",
      "Iteration 6290 : Loss 2797.0547\n",
      "Iteration 6300 : Loss 2797.0448\n",
      "Iteration 6310 : Loss 2797.0349\n",
      "Iteration 6320 : Loss 2797.0251\n",
      "Iteration 6330 : Loss 2797.0154\n",
      "Iteration 6340 : Loss 2797.0057\n",
      "Iteration 6350 : Loss 2796.9960\n",
      "Iteration 6360 : Loss 2796.9864\n",
      "Iteration 6370 : Loss 2796.9769\n",
      "Iteration 6380 : Loss 2796.9674\n",
      "Iteration 6390 : Loss 2796.9580\n",
      "Iteration 6400 : Loss 2796.9486\n",
      "Iteration 6410 : Loss 2796.9392\n",
      "Iteration 6420 : Loss 2796.9299\n",
      "Iteration 6430 : Loss 2796.9207\n",
      "Iteration 6440 : Loss 2796.9115\n",
      "Iteration 6450 : Loss 2796.9024\n",
      "Iteration 6460 : Loss 2796.8933\n",
      "Iteration 6470 : Loss 2796.8843\n",
      "Iteration 6480 : Loss 2796.8753\n",
      "Iteration 6490 : Loss 2796.8664\n",
      "Iteration 6500 : Loss 2796.8575\n",
      "Iteration 6510 : Loss 2796.8486\n",
      "Iteration 6520 : Loss 2796.8399\n",
      "Iteration 6530 : Loss 2796.8311\n",
      "Iteration 6540 : Loss 2796.8224\n",
      "Iteration 6550 : Loss 2796.8138\n",
      "Iteration 6560 : Loss 2796.8052\n",
      "Iteration 6570 : Loss 2796.7966\n",
      "Iteration 6580 : Loss 2796.7881\n",
      "Iteration 6590 : Loss 2796.7796\n",
      "Iteration 6600 : Loss 2796.7712\n",
      "Iteration 6610 : Loss 2796.7628\n",
      "Iteration 6620 : Loss 2796.7545\n",
      "Iteration 6630 : Loss 2796.7462\n",
      "Iteration 6640 : Loss 2796.7380\n",
      "Iteration 6650 : Loss 2796.7298\n",
      "Iteration 6660 : Loss 2796.7216\n",
      "Iteration 6670 : Loss 2796.7135\n",
      "Iteration 6680 : Loss 2796.7054\n",
      "Iteration 6690 : Loss 2796.6974\n",
      "Iteration 6700 : Loss 2796.6894\n",
      "Iteration 6710 : Loss 2796.6815\n",
      "Iteration 6720 : Loss 2796.6736\n",
      "Iteration 6730 : Loss 2796.6657\n",
      "Iteration 6740 : Loss 2796.6579\n",
      "Iteration 6750 : Loss 2796.6501\n",
      "Iteration 6760 : Loss 2796.6424\n",
      "Iteration 6770 : Loss 2796.6347\n",
      "Iteration 6780 : Loss 2796.6270\n",
      "Iteration 6790 : Loss 2796.6194\n",
      "Iteration 6800 : Loss 2796.6118\n",
      "Iteration 6810 : Loss 2796.6043\n",
      "Iteration 6820 : Loss 2796.5968\n",
      "Iteration 6830 : Loss 2796.5893\n",
      "Iteration 6840 : Loss 2796.5819\n",
      "Iteration 6850 : Loss 2796.5745\n",
      "Iteration 6860 : Loss 2796.5672\n",
      "Iteration 6870 : Loss 2796.5599\n",
      "Iteration 6880 : Loss 2796.5526\n",
      "Iteration 6890 : Loss 2796.5454\n",
      "Iteration 6900 : Loss 2796.5382\n",
      "Iteration 6910 : Loss 2796.5310\n",
      "Iteration 6920 : Loss 2796.5239\n",
      "Iteration 6930 : Loss 2796.5168\n",
      "Iteration 6940 : Loss 2796.5097\n",
      "Iteration 6950 : Loss 2796.5027\n",
      "Iteration 6960 : Loss 2796.4957\n",
      "Iteration 6970 : Loss 2796.4888\n",
      "Iteration 6980 : Loss 2796.4818\n",
      "Iteration 6990 : Loss 2796.4750\n",
      "Iteration 7000 : Loss 2796.4681\n",
      "Iteration 7010 : Loss 2796.4613\n",
      "Iteration 7020 : Loss 2796.4545\n",
      "Iteration 7030 : Loss 2796.4478\n",
      "Iteration 7040 : Loss 2796.4411\n",
      "Iteration 7050 : Loss 2796.4344\n",
      "Iteration 7060 : Loss 2796.4277\n",
      "Iteration 7070 : Loss 2796.4211\n",
      "Iteration 7080 : Loss 2796.4146\n",
      "Iteration 7090 : Loss 2796.4080\n",
      "Iteration 7100 : Loss 2796.4015\n",
      "Iteration 7110 : Loss 2796.3950\n",
      "Iteration 7120 : Loss 2796.3886\n",
      "Iteration 7130 : Loss 2796.3821\n",
      "Iteration 7140 : Loss 2796.3757\n",
      "Iteration 7150 : Loss 2796.3694\n",
      "Iteration 7160 : Loss 2796.3631\n",
      "Iteration 7170 : Loss 2796.3568\n",
      "Iteration 7180 : Loss 2796.3505\n",
      "Iteration 7190 : Loss 2796.3443\n",
      "Iteration 7200 : Loss 2796.3380\n",
      "Iteration 7210 : Loss 2796.3319\n",
      "Iteration 7220 : Loss 2796.3257\n",
      "Iteration 7230 : Loss 2796.3196\n",
      "Iteration 7240 : Loss 2796.3135\n",
      "Iteration 7250 : Loss 2796.3075\n",
      "Iteration 7260 : Loss 2796.3014\n",
      "Iteration 7270 : Loss 2796.2954\n",
      "Iteration 7280 : Loss 2796.2894\n",
      "Iteration 7290 : Loss 2796.2835\n",
      "Iteration 7300 : Loss 2796.2776\n",
      "Iteration 7310 : Loss 2796.2717\n",
      "Iteration 7320 : Loss 2796.2658\n",
      "Iteration 7330 : Loss 2796.2600\n",
      "Iteration 7340 : Loss 2796.2542\n",
      "Iteration 7350 : Loss 2796.2484\n",
      "Iteration 7360 : Loss 2796.2426\n",
      "Iteration 7370 : Loss 2796.2369\n",
      "Iteration 7380 : Loss 2796.2312\n",
      "Iteration 7390 : Loss 2796.2255\n",
      "Iteration 7400 : Loss 2796.2199\n",
      "Iteration 7410 : Loss 2796.2143\n",
      "Iteration 7420 : Loss 2796.2087\n",
      "Iteration 7430 : Loss 2796.2031\n",
      "Iteration 7440 : Loss 2796.1976\n",
      "Iteration 7450 : Loss 2796.1920\n",
      "Iteration 7460 : Loss 2796.1865\n",
      "Iteration 7470 : Loss 2796.1811\n",
      "Iteration 7480 : Loss 2796.1756\n",
      "Iteration 7490 : Loss 2796.1702\n",
      "Iteration 7500 : Loss 2796.1648\n",
      "Iteration 7510 : Loss 2796.1594\n",
      "Iteration 7520 : Loss 2796.1541\n",
      "Iteration 7530 : Loss 2796.1488\n",
      "Iteration 7540 : Loss 2796.1435\n",
      "Iteration 7550 : Loss 2796.1382\n",
      "Iteration 7560 : Loss 2796.1329\n",
      "Iteration 7570 : Loss 2796.1277\n",
      "Iteration 7580 : Loss 2796.1225\n",
      "Iteration 7590 : Loss 2796.1173\n",
      "Iteration 7600 : Loss 2796.1122\n",
      "Iteration 7610 : Loss 2796.1070\n",
      "Iteration 7620 : Loss 2796.1019\n",
      "Iteration 7630 : Loss 2796.0968\n",
      "Iteration 7640 : Loss 2796.0918\n",
      "Iteration 7650 : Loss 2796.0867\n",
      "Iteration 7660 : Loss 2796.0817\n",
      "Iteration 7670 : Loss 2796.0767\n",
      "Iteration 7680 : Loss 2796.0717\n",
      "Iteration 7690 : Loss 2796.0667\n",
      "Iteration 7700 : Loss 2796.0618\n",
      "Iteration 7710 : Loss 2796.0569\n",
      "Iteration 7720 : Loss 2796.0520\n",
      "Iteration 7730 : Loss 2796.0471\n",
      "Iteration 7740 : Loss 2796.0423\n",
      "Iteration 7750 : Loss 2796.0374\n",
      "Iteration 7760 : Loss 2796.0326\n",
      "Iteration 7770 : Loss 2796.0278\n",
      "Iteration 7780 : Loss 2796.0230\n",
      "Iteration 7790 : Loss 2796.0183\n",
      "Iteration 7800 : Loss 2796.0136\n",
      "Iteration 7810 : Loss 2796.0089\n",
      "Iteration 7820 : Loss 2796.0042\n",
      "Iteration 7830 : Loss 2795.9995\n",
      "Iteration 7840 : Loss 2795.9948\n",
      "Iteration 7850 : Loss 2795.9902\n",
      "Iteration 7860 : Loss 2795.9856\n",
      "Iteration 7870 : Loss 2795.9810\n",
      "Iteration 7880 : Loss 2795.9764\n",
      "Iteration 7890 : Loss 2795.9719\n",
      "Iteration 7900 : Loss 2795.9673\n",
      "Iteration 7910 : Loss 2795.9628\n",
      "Iteration 7920 : Loss 2795.9583\n",
      "Iteration 7930 : Loss 2795.9538\n",
      "Iteration 7940 : Loss 2795.9494\n",
      "Iteration 7950 : Loss 2795.9449\n",
      "Iteration 7960 : Loss 2795.9405\n",
      "Iteration 7970 : Loss 2795.9361\n",
      "Iteration 7980 : Loss 2795.9317\n",
      "Iteration 7990 : Loss 2795.9273\n",
      "Iteration 8000 : Loss 2795.9230\n",
      "Iteration 8010 : Loss 2795.9186\n",
      "Iteration 8020 : Loss 2795.9143\n",
      "Iteration 8030 : Loss 2795.9100\n",
      "Iteration 8040 : Loss 2795.9057\n",
      "Iteration 8050 : Loss 2795.9014\n",
      "Iteration 8060 : Loss 2795.8972\n",
      "Iteration 8070 : Loss 2795.8930\n",
      "Iteration 8080 : Loss 2795.8887\n",
      "Iteration 8090 : Loss 2795.8845\n",
      "Iteration 8100 : Loss 2795.8804\n",
      "Iteration 8110 : Loss 2795.8762\n",
      "Iteration 8120 : Loss 2795.8720\n",
      "Iteration 8130 : Loss 2795.8679\n",
      "Iteration 8140 : Loss 2795.8638\n",
      "Iteration 8150 : Loss 2795.8597\n",
      "Iteration 8160 : Loss 2795.8556\n",
      "Iteration 8170 : Loss 2795.8515\n",
      "Iteration 8180 : Loss 2795.8475\n",
      "Iteration 8190 : Loss 2795.8434\n",
      "Iteration 8200 : Loss 2795.8394\n",
      "Iteration 8210 : Loss 2795.8354\n",
      "Iteration 8220 : Loss 2795.8314\n",
      "Iteration 8230 : Loss 2795.8274\n",
      "Iteration 8240 : Loss 2795.8234\n",
      "Iteration 8250 : Loss 2795.8195\n",
      "Iteration 8260 : Loss 2795.8156\n",
      "Iteration 8270 : Loss 2795.8116\n",
      "Iteration 8280 : Loss 2795.8077\n",
      "Iteration 8290 : Loss 2795.8038\n",
      "Iteration 8300 : Loss 2795.8000\n",
      "Iteration 8310 : Loss 2795.7961\n",
      "Iteration 8320 : Loss 2795.7923\n",
      "Iteration 8330 : Loss 2795.7884\n",
      "Iteration 8340 : Loss 2795.7846\n",
      "Iteration 8350 : Loss 2795.7808\n",
      "Iteration 8360 : Loss 2795.7770\n",
      "Iteration 8370 : Loss 2795.7732\n",
      "Iteration 8380 : Loss 2795.7695\n",
      "Iteration 8390 : Loss 2795.7657\n",
      "Iteration 8400 : Loss 2795.7620\n",
      "Iteration 8410 : Loss 2795.7583\n",
      "Iteration 8420 : Loss 2795.7546\n",
      "Iteration 8430 : Loss 2795.7509\n",
      "Iteration 8440 : Loss 2795.7472\n",
      "Iteration 8450 : Loss 2795.7435\n",
      "Iteration 8460 : Loss 2795.7399\n",
      "Iteration 8470 : Loss 2795.7362\n",
      "Iteration 8480 : Loss 2795.7326\n",
      "Iteration 8490 : Loss 2795.7290\n",
      "Iteration 8500 : Loss 2795.7254\n",
      "Iteration 8510 : Loss 2795.7218\n",
      "Iteration 8520 : Loss 2795.7182\n",
      "Iteration 8530 : Loss 2795.7146\n",
      "Iteration 8540 : Loss 2795.7111\n",
      "Iteration 8550 : Loss 2795.7075\n",
      "Iteration 8560 : Loss 2795.7040\n",
      "Iteration 8570 : Loss 2795.7005\n",
      "Iteration 8580 : Loss 2795.6970\n",
      "Iteration 8590 : Loss 2795.6935\n",
      "Iteration 8600 : Loss 2795.6900\n",
      "Iteration 8610 : Loss 2795.6866\n",
      "Iteration 8620 : Loss 2795.6831\n",
      "Iteration 8630 : Loss 2795.6797\n",
      "Iteration 8640 : Loss 2795.6762\n",
      "Iteration 8650 : Loss 2795.6728\n",
      "Iteration 8660 : Loss 2795.6694\n",
      "Iteration 8670 : Loss 2795.6660\n",
      "Iteration 8680 : Loss 2795.6626\n",
      "Iteration 8690 : Loss 2795.6592\n",
      "Iteration 8700 : Loss 2795.6559\n",
      "Iteration 8710 : Loss 2795.6525\n",
      "Iteration 8720 : Loss 2795.6492\n",
      "Iteration 8730 : Loss 2795.6459\n",
      "Iteration 8740 : Loss 2795.6425\n",
      "Iteration 8750 : Loss 2795.6392\n",
      "Iteration 8760 : Loss 2795.6359\n",
      "Iteration 8770 : Loss 2795.6327\n",
      "Iteration 8780 : Loss 2795.6294\n",
      "Iteration 8790 : Loss 2795.6261\n",
      "Iteration 8800 : Loss 2795.6229\n",
      "Iteration 8810 : Loss 2795.6196\n",
      "Iteration 8820 : Loss 2795.6164\n",
      "Iteration 8830 : Loss 2795.6132\n",
      "Iteration 8840 : Loss 2795.6100\n",
      "Iteration 8850 : Loss 2795.6068\n",
      "Iteration 8860 : Loss 2795.6036\n",
      "Iteration 8870 : Loss 2795.6004\n",
      "Iteration 8880 : Loss 2795.5972\n",
      "Iteration 8890 : Loss 2795.5941\n",
      "Iteration 8900 : Loss 2795.5909\n",
      "Iteration 8910 : Loss 2795.5878\n",
      "Iteration 8920 : Loss 2795.5846\n",
      "Iteration 8930 : Loss 2795.5815\n",
      "Iteration 8940 : Loss 2795.5784\n",
      "Iteration 8950 : Loss 2795.5753\n",
      "Iteration 8960 : Loss 2795.5722\n",
      "Iteration 8970 : Loss 2795.5691\n",
      "Iteration 8980 : Loss 2795.5660\n",
      "Iteration 8990 : Loss 2795.5630\n",
      "Iteration 9000 : Loss 2795.5599\n",
      "Iteration 9010 : Loss 2795.5569\n",
      "Iteration 9020 : Loss 2795.5538\n",
      "Iteration 9030 : Loss 2795.5508\n",
      "Iteration 9040 : Loss 2795.5478\n",
      "Iteration 9050 : Loss 2795.5448\n",
      "Iteration 9060 : Loss 2795.5418\n",
      "Iteration 9070 : Loss 2795.5388\n",
      "Iteration 9080 : Loss 2795.5358\n",
      "Iteration 9090 : Loss 2795.5329\n",
      "Iteration 9100 : Loss 2795.5299\n",
      "Iteration 9110 : Loss 2795.5269\n",
      "Iteration 9120 : Loss 2795.5240\n",
      "Iteration 9130 : Loss 2795.5211\n",
      "Iteration 9140 : Loss 2795.5181\n",
      "Iteration 9150 : Loss 2795.5152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9160 : Loss 2795.5123\n",
      "Iteration 9170 : Loss 2795.5094\n",
      "Iteration 9180 : Loss 2795.5065\n",
      "Iteration 9190 : Loss 2795.5036\n",
      "Iteration 9200 : Loss 2795.5007\n",
      "Iteration 9210 : Loss 2795.4979\n",
      "Iteration 9220 : Loss 2795.4950\n",
      "Iteration 9230 : Loss 2795.4921\n",
      "Iteration 9240 : Loss 2795.4893\n",
      "Iteration 9250 : Loss 2795.4865\n",
      "Iteration 9260 : Loss 2795.4836\n",
      "Iteration 9270 : Loss 2795.4808\n",
      "Iteration 9280 : Loss 2795.4780\n",
      "Iteration 9290 : Loss 2795.4752\n",
      "Iteration 9300 : Loss 2795.4724\n",
      "Iteration 9310 : Loss 2795.4696\n",
      "Iteration 9320 : Loss 2795.4668\n",
      "Iteration 9330 : Loss 2795.4641\n",
      "Iteration 9340 : Loss 2795.4613\n",
      "Iteration 9350 : Loss 2795.4585\n",
      "Iteration 9360 : Loss 2795.4558\n",
      "Iteration 9370 : Loss 2795.4530\n",
      "Iteration 9380 : Loss 2795.4503\n",
      "Iteration 9390 : Loss 2795.4476\n",
      "Iteration 9400 : Loss 2795.4448\n",
      "Iteration 9410 : Loss 2795.4421\n",
      "Iteration 9420 : Loss 2795.4394\n",
      "Iteration 9430 : Loss 2795.4367\n",
      "Iteration 9440 : Loss 2795.4340\n",
      "Iteration 9450 : Loss 2795.4313\n",
      "Iteration 9460 : Loss 2795.4287\n",
      "Iteration 9470 : Loss 2795.4260\n",
      "Iteration 9480 : Loss 2795.4233\n",
      "Iteration 9490 : Loss 2795.4207\n",
      "Iteration 9500 : Loss 2795.4180\n",
      "Iteration 9510 : Loss 2795.4154\n",
      "Iteration 9520 : Loss 2795.4127\n",
      "Iteration 9530 : Loss 2795.4101\n",
      "Iteration 9540 : Loss 2795.4075\n",
      "Iteration 9550 : Loss 2795.4049\n",
      "Iteration 9560 : Loss 2795.4023\n",
      "Iteration 9570 : Loss 2795.3997\n",
      "Iteration 9580 : Loss 2795.3971\n",
      "Iteration 9590 : Loss 2795.3945\n",
      "Iteration 9600 : Loss 2795.3919\n",
      "Iteration 9610 : Loss 2795.3893\n",
      "Iteration 9620 : Loss 2795.3867\n",
      "Iteration 9630 : Loss 2795.3842\n",
      "Iteration 9640 : Loss 2795.3816\n",
      "Iteration 9650 : Loss 2795.3791\n",
      "Iteration 9660 : Loss 2795.3765\n",
      "Iteration 9670 : Loss 2795.3740\n",
      "Iteration 9680 : Loss 2795.3714\n",
      "Iteration 9690 : Loss 2795.3689\n",
      "Iteration 9700 : Loss 2795.3664\n",
      "Iteration 9710 : Loss 2795.3639\n",
      "Iteration 9720 : Loss 2795.3614\n",
      "Iteration 9730 : Loss 2795.3589\n",
      "Iteration 9740 : Loss 2795.3564\n",
      "Iteration 9750 : Loss 2795.3539\n",
      "Iteration 9760 : Loss 2795.3514\n",
      "Iteration 9770 : Loss 2795.3489\n",
      "Iteration 9780 : Loss 2795.3464\n",
      "Iteration 9790 : Loss 2795.3440\n",
      "Iteration 9800 : Loss 2795.3415\n",
      "Iteration 9810 : Loss 2795.3391\n",
      "Iteration 9820 : Loss 2795.3366\n",
      "Iteration 9830 : Loss 2795.3342\n",
      "Iteration 9840 : Loss 2795.3317\n",
      "Iteration 9850 : Loss 2795.3293\n",
      "Iteration 9860 : Loss 2795.3269\n",
      "Iteration 9870 : Loss 2795.3244\n",
      "Iteration 9880 : Loss 2795.3220\n",
      "Iteration 9890 : Loss 2795.3196\n",
      "Iteration 9900 : Loss 2795.3172\n",
      "Iteration 9910 : Loss 2795.3148\n",
      "Iteration 9920 : Loss 2795.3124\n",
      "Iteration 9930 : Loss 2795.3100\n",
      "Iteration 9940 : Loss 2795.3077\n",
      "Iteration 9950 : Loss 2795.3053\n",
      "Iteration 9960 : Loss 2795.3029\n",
      "Iteration 9970 : Loss 2795.3005\n",
      "Iteration 9980 : Loss 2795.2982\n",
      "Iteration 9990 : Loss 2795.2958\n",
      "Iteration 10000 : Loss 2795.2935\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for i in range(1, 10001):\n",
    "    dW, db = gradient(X_train, W, b, y_train)\n",
    "    W -= learning_rate * dW\n",
    "    b -= learning_rate * db\n",
    "    L = loss(X_train, W, b, y_train)\n",
    "    losses.append(L)\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i, L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9006bba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prediction = model(X_test, W, b)\n",
    "plt.scatter(X_test[:, 0], y_test)\n",
    "plt.scatter(X_test[:, 0], prediction)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
